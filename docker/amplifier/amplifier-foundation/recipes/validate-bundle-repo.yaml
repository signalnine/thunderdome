# validate-bundle-repo.yaml
# Repository-Wide Bundle Validator Recipe v3.2.0
# Validates an entire bundle repository against structural requirements, conventions,
# and Amplifier bundle philosophy (context sink pattern, thin behaviors, tool placement)
#
# CHANGELOG:
# v3.2.0 - Graceful environment handling
#        - Phase 0 now checks prerequisites without auto-install attempts
#        - Detects hatchling, amplifier_foundation, pip wheel capability
#        - Sets validation_mode: full | hygiene_only | structural_only
#        - Build check gracefully SKIPS when hatchling unavailable (not ERROR)
#        - Foundation unavailability no longer classified as "critical"
#        - New "Environment Status" section in reports
#        - Clear separation of "environment limitations" vs "bundle problems"
#        - Instructions provided for achieving full validation
#
# v3.1.0 - Inheritance-aware tool placement analysis
#        - Phase 2.5 now parses delegate tool's exclude_tools config
#        - Agents only flagged for redundant tools if they INHERIT them (not excluded)
#        - Tools in exclude_tools list require explicit agent declaration (CORRECT pattern)
#        - Default exclude_tools: ["delegate"] - agents correctly redeclare delegate if needed
#        - Removed naive "redundant declaration" flagging that didn't understand inheritance
#
# v3.0.0 - Major upgrade with comprehensive bundle hygiene validation
#        - Enhanced Phase 1: Separate tracking for behaviors/standalone/experiments/root
#        - Phase 2.1: Behavior Hygiene Validation (no root includes, no session config, context limits)
#        - Phase 2.2: Standalone Completeness Validation (orchestrator + context required)
#        - Phase 2.3: Experiments Validation (naming, completeness, EXPERIMENTAL warning)
#        - Phase 2.4: Context Sink Compliance (root bundle context bloat detection)
#        - Phase 2.5: Tool Placement Analysis (universal vs capability vs specialized)
#        - Enhanced quality classification incorporating all new phases
#        - Enhanced final report with new validation sections
#        - CRITICAL: All size measurements use TOKENS (len/4), never lines
#
# v2.1.0 - Added Python packaging validation phases
#        - Phase 0.5: packaging-check (pyproject.toml force-includes)
#        - Phase 0.7: build-check (dry-run pip wheel)
#
# v2.0.0 - Added explicit PASS thresholds and deterministic quality classification
#
# v1.0.0 - Initial release
#
# PASS THRESHOLDS (explicit criteria for "passing"):
# 1. All bundles load without errors (BundleRegistry succeeds)
# 2. Root bundle exists (if any bundles in repo) OR has behaviors/bundles dirs
# 3. No orphan agents (all agents referenced by some bundle)
# 4. No broken cross-bundle references (includes resolve)
# 5. Consistent namespace across bundles (if multiple bundles)
# 6. Python packaging builds successfully (if pyproject.toml present)
# 7. Behaviors don't include root bundles (context sink anti-pattern)
# 8. Behaviors have reasonable context.include token counts (<2000 ERROR, <1000 WARNING)
# 9. Standalone bundles in /bundles/ have complete session configuration
# 10. Experimental bundles follow naming conventions
#
# QUALITY LEVELS:
# - good: All bundles load, no errors, no orphans, proper structure, philosophy aligned
# - polish: All load but has warnings (missing descriptions, conventions, minor hygiene)
# - needs_work: Orphan agents, missing root, convention issues, hygiene violations
# - critical: Any bundle fails to load OR Python package fails to build
#
# TOKEN ESTIMATION:
# All size measurements use tokens estimated as: len(content) // 4
# This provides consistent sizing regardless of line length variations.

name: validate-bundle-repo
description: |
  Validates an entire Amplifier bundle repository including:
  - Discovery of all bundle files (root, behaviors, standalone, experiments, providers)
  - Python packaging validation (pyproject.toml force-includes, build check)
  - Individual validation of each bundle via BundleRegistry
  - **NEW in v3.0.0:** Behavior hygiene validation (no root includes, context limits)
  - **NEW in v3.0.0:** Standalone bundle completeness validation
  - **NEW in v3.0.0:** Experiments validation (naming, completeness)
  - **NEW in v3.0.0:** Context sink compliance (root bundle context bloat)
  - **NEW in v3.0.0:** Tool placement analysis (universal vs specialized)
  - Deterministic quality classification with explicit PASS thresholds
  - Quick-approval path for clean repos (no unnecessary LLM analysis)
  - Repository composition analysis (do pieces fit together correctly?)
  - Convention compliance across the whole repo
  
  **v3.2.0 Enhancements: Graceful Environment Handling**
  - Prerequisite check without auto-install attempts (predictable, fast)
  - Validation modes: full | hygiene_only | structural_only
  - Build check gracefully SKIPs when hatchling unavailable (INFO, not ERROR)
  - Foundation unavailability → limited validation mode (not "critical")
  - Clear "Environment Status" section in all reports
  - Separation of "environment limitations" vs "bundle problems"
  
  **Enabling Full Validation (if deps missing)**
  
  If running on a system with externally-managed Python (PEP 668), use uvx:
  ```bash
  # Install deps ephemerally via uvx
  uvx --with hatchling --with "git+https://github.com/microsoft/amplifier-foundation@main" \
      amplifier tool invoke recipes operation=execute \
      recipe_path=foundation:recipes/validate-bundle-repo.yaml \
      context='{"repo_path": "/path/to/repo"}'
  ```
  
  Or install directly (if pip works):
  ```bash
  pip install hatchling
  pip install git+https://github.com/microsoft/amplifier-foundation@main
  ```
  
  **v3.1.0 Enhancements:**
  - Inheritance-aware tool placement analysis
  - Understands delegate tool's exclude_tools config (default: ["delegate"])
  - Only flags agent tool declarations as redundant if they're actually inherited
  - Tools in exclude_tools correctly require explicit agent declaration
  
  **v3.0.0 Philosophy Alignment:**
  - Enforces context sink pattern (heavy context → agents, not root/behaviors)
  - Enforces thin behavior pattern (behaviors should be small, focused)
  - Enforces tool placement philosophy (universal → root, specialized → agents)
  - All measurements use TOKENS (len/4), never line counts
  
  For single bundle validation, use validate-bundle.yaml instead.
version: "3.2.0"
author: "Amplifier Foundation Team"
tags: ["bundle", "validation", "quality", "conventions", "repository", "packaging", "hygiene", "context-sink"]

context:
  repo_path: ""  # Required: Path to bundle repository root
  validate_agents: "false"  # Optional: Also run agent validation (true/false)

steps:
  # ============================================================================
  # PHASE 0: Environment Prerequisite Check (v3.2.0)
  # Check available tools WITHOUT auto-install attempts
  # Sets validation_mode: full | hygiene_only | structural_only
  # ============================================================================

  - id: "environment-check"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      import sys
      import subprocess

      results = {
          "phase": "environment_prerequisites",
          "python_version": sys.version.split()[0],
          "capabilities": {
              "foundation_available": False,
              "foundation_version": None,
              "hatchling_available": False,
              "pip_wheel_available": False
          },
          "validation_mode": "structural_only",  # Default to most limited
          "limitations": [],
          "install_instructions": []
      }

      # Check 1: amplifier_foundation (enables BundleRegistry validation)
      try:
          import amplifier_foundation
          results["capabilities"]["foundation_available"] = True
          results["capabilities"]["foundation_version"] = getattr(amplifier_foundation, "__version__", "unknown")
      except ImportError:
          results["limitations"].append({
              "component": "amplifier_foundation",
              "impact": "Cannot validate individual bundles via BundleRegistry",
              "skipped_checks": ["individual_validation", "standalone_completeness (full)"]
          })
          # Note: amplifier-core must be installed first (foundation depends on it)

      # Check 2: hatchling (enables Python package build validation)
      try:
          import hatchling
          results["capabilities"]["hatchling_available"] = True
      except ImportError:
          results["limitations"].append({
              "component": "hatchling",
              "impact": "Cannot validate Python package builds",
              "skipped_checks": ["build_check"]
          })

      # Build comprehensive install instructions if anything is missing
      foundation_available = results["capabilities"]["foundation_available"]
      hatchling_available = results["capabilities"]["hatchling_available"]
      
      if not foundation_available or not hatchling_available:
          results["install_instructions"] = {
              "summary": "Install missing dependencies, then re-run this recipe for full validation.",
              "standard": [],
              "pep668": []
          }
          
          # Standard pip commands (in correct order)
          if not hatchling_available:
              results["install_instructions"]["standard"].append("pip install hatchling")
          if not foundation_available:
              results["install_instructions"]["standard"].append(
                  "pip install git+https://github.com/microsoft/amplifier-core@main"
              )
              results["install_instructions"]["standard"].append(
                  "pip install git+https://github.com/microsoft/amplifier-foundation@main"
              )
          
          # PEP 668 commands (with --break-system-packages)
          if not hatchling_available:
              results["install_instructions"]["pep668"].append(
                  "pip install --break-system-packages hatchling"
              )
          if not foundation_available:
              results["install_instructions"]["pep668"].append(
                  "pip install --break-system-packages git+https://github.com/microsoft/amplifier-core@main"
              )
              results["install_instructions"]["pep668"].append(
                  "pip install --break-system-packages git+https://github.com/microsoft/amplifier-foundation@main"
              )

      # Check 3: pip wheel capability
      try:
          proc = subprocess.run(
              [sys.executable, "-m", "pip", "wheel", "--help"],
              capture_output=True,
              timeout=10
          )
          results["capabilities"]["pip_wheel_available"] = proc.returncode == 0
      except Exception:
          results["capabilities"]["pip_wheel_available"] = False

      if not results["capabilities"]["pip_wheel_available"]:
          results["limitations"].append({
              "component": "pip wheel",
              "impact": "Cannot run build dry-run",
              "skipped_checks": ["build_check"]
          })

      # Determine validation mode based on capabilities
      foundation = results["capabilities"]["foundation_available"]
      hatchling = results["capabilities"]["hatchling_available"]

      if foundation and hatchling:
          results["validation_mode"] = "full"
          results["mode_description"] = "Full validation: all checks enabled"
      elif foundation:
          results["validation_mode"] = "full_no_build"
          results["mode_description"] = "Full validation without build check (hatchling unavailable)"
      elif hatchling:
          results["validation_mode"] = "hygiene_only"
          results["mode_description"] = "Hygiene-only: structural checks without BundleRegistry validation"
      else:
          results["validation_mode"] = "structural_only"
          results["mode_description"] = "Structural-only: basic file checks without dependency validation"

      # Summary
      results["summary"] = {
          "can_validate_bundles": foundation,
          "can_validate_builds": hatchling and results["capabilities"]["pip_wheel_available"],
          "limitations_count": len(results["limitations"]),
          "full_validation_available": foundation and hatchling
      }

      # Note: We do NOT auto-install anything. This is intentional for:
      # - Predictability (no surprise environment changes)
      # - Speed (no waiting for installs)
      # - Permissions (may not have install rights)
      # - Isolation (don't pollute environments)

      print(json.dumps(results))
      EOF
    output: "env_check"
    parse_json: true
    timeout: 30

  # ============================================================================
  # PHASE 0.5: Packaging Consistency Check
  # Verify pyproject.toml force-includes reference existing directories
  # ============================================================================

  - id: "packaging-check"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      import sys
      from pathlib import Path

      repo_path = "{{repo_path}}"

      result = {
          "phase": "packaging_check",
          "has_pyproject": False,
          "force_includes_checked": 0,
          "errors": [],
          "warnings": []
      }

      pyproject_path = Path(repo_path) / "pyproject.toml"

      if not pyproject_path.exists():
          result["skipped"] = True
          result["passed"] = True
          result["reason"] = "No pyproject.toml (not a Python package bundle)"
          print(json.dumps(result))
          sys.exit(0)

      result["has_pyproject"] = True

      try:
          import tomllib
      except ImportError:
          try:
              import tomli as tomllib
          except ImportError:
              result["warnings"].append({
                  "type": "missing_toml_parser",
                  "message": "Cannot parse pyproject.toml - tomllib/tomli not available"
              })
              result["passed"] = True
              print(json.dumps(result))
              sys.exit(0)

      try:
          with open(pyproject_path, "rb") as f:
              config = tomllib.load(f)
      except Exception as e:
          result["errors"].append({
              "type": "toml_parse_error",
              "message": f"Failed to parse pyproject.toml: {e}",
              "severity": "ERROR"
          })
          result["passed"] = False
          print(json.dumps(result))
          sys.exit(0)

      force_includes = (config.get("tool", {})
                             .get("hatch", {})
                             .get("build", {})
                             .get("targets", {})
                             .get("wheel", {})
                             .get("force-include", {}))

      for source_path, dest_path in force_includes.items():
          result["force_includes_checked"] += 1
          full_path = Path(repo_path) / source_path

          if not full_path.exists():
              result["errors"].append({
                  "type": "missing_force_include",
                  "source": source_path,
                  "destination": dest_path,
                  "message": f"pyproject.toml force-include references non-existent path: {source_path}",
                  "severity": "CRITICAL",
                  "fix": f"Either create '{source_path}' or remove/update the force-include entry"
              })

      package_data = config.get("tool", {}).get("setuptools", {}).get("package-data", {})
      for package, patterns in package_data.items():
          for pattern in patterns:
              if "*" not in pattern and "?" not in pattern:
                  full_path = Path(repo_path) / pattern
                  if not full_path.exists():
                      result["warnings"].append({
                          "type": "missing_package_data",
                          "package": package,
                          "pattern": pattern,
                          "message": f"package-data references potentially missing path: {pattern}"
                      })

      result["passed"] = len(result["errors"]) == 0
      print(json.dumps(result))
      EOF
    output: "packaging_check"
    parse_json: true
    timeout: 30
    depends_on: ["environment-check"]

  # ============================================================================
  # PHASE 0.7: Build Dry-Run Check (v3.2.0 - Graceful Degradation)
  # Now gracefully SKIPs when hatchling unavailable (INFO, not ERROR)
  # ============================================================================

  - id: "build-check"
    type: "bash"
    condition: "{{packaging_check.has_pyproject}} == true"
    command: |
      python3 << 'EOF'
      import json
      import subprocess
      import sys
      import tempfile
      from pathlib import Path

      repo_path = "{{repo_path}}"
      env_check = json.loads('''{{env_check}}''')

      result = {
          "phase": "build_check",
          "build_tested": False,
          "errors": [],
          "warnings": [],
          "info": []
      }

      pyproject_path = Path(repo_path) / "pyproject.toml"
      if not pyproject_path.exists():
          result["skipped"] = True
          result["passed"] = True
          result["reason"] = "No pyproject.toml"
          print(json.dumps(result))
          sys.exit(0)

      # v3.2.0: Check if build tools are available BEFORE attempting build
      hatchling_available = env_check.get("capabilities", {}).get("hatchling_available", False)
      pip_wheel_available = env_check.get("capabilities", {}).get("pip_wheel_available", False)

      if not hatchling_available:
          result["skipped"] = True
          result["passed"] = True  # Not a failure - just can't test
          result["reason"] = "hatchling build backend not available"
          result["info"].append({
              "type": "build_tools_unavailable",
              "message": "Build check skipped: hatchling not installed",
              "severity": "INFO",
              "suggestion": "Install hatchling for full build validation: pip install hatchling"
          })
          result["environment_limitation"] = True
          print(json.dumps(result))
          sys.exit(0)

      if not pip_wheel_available:
          result["skipped"] = True
          result["passed"] = True
          result["reason"] = "pip wheel not available"
          result["info"].append({
              "type": "pip_wheel_unavailable",
              "message": "Build check skipped: pip wheel command not available",
              "severity": "INFO"
          })
          result["environment_limitation"] = True
          print(json.dumps(result))
          sys.exit(0)

      # Build tools available - proceed with actual build test
      try:
          with tempfile.TemporaryDirectory() as tmpdir:
              proc = subprocess.run(
                  [sys.executable, "-m", "pip", "wheel", "--no-deps", "--no-build-isolation",
                   "-w", tmpdir, repo_path],
                  capture_output=True,
                  text=True,
                  timeout=120,
                  cwd=repo_path
              )

              if proc.returncode != 0:
                  error_output = proc.stderr or proc.stdout

                  # v3.2.0: Check if this is ACTUALLY a missing hatchling error
                  # (can happen even if import worked, due to version mismatch)
                  if "hatchling" in error_output.lower() and ("not found" in error_output.lower() or "no module" in error_output.lower()):
                      result["skipped"] = True
                      result["passed"] = True
                      result["reason"] = "hatchling not properly configured for this project"
                      result["info"].append({
                          "type": "hatchling_mismatch",
                          "message": "Build backend available but not compatible with this project",
                          "severity": "INFO",
                          "details": error_output[-500:]
                      })
                      result["environment_limitation"] = True
                  elif "FileNotFoundError" in error_output or "No such file or directory" in error_output:
                      # This IS a bundle problem - missing file referenced in config
                      result["errors"].append({
                          "type": "build_missing_file",
                          "message": "Build failed due to missing file (bundle configuration issue)",
                          "details": error_output[-2000:],
                          "severity": "ERROR"
                      })
                  elif "force-include" in error_output.lower():
                      result["errors"].append({
                          "type": "build_force_include_error",
                          "message": "Build failed due to force-include configuration",
                          "details": error_output[-2000:],
                          "severity": "ERROR"
                      })
                  else:
                      result["errors"].append({
                          "type": "build_error",
                          "message": "Package build failed",
                          "details": error_output[-2000:],
                          "severity": "ERROR"
                      })
              else:
                  result["build_tested"] = True
                  result["build_success"] = True

      except subprocess.TimeoutExpired:
          result["warnings"].append({
              "type": "build_timeout",
              "message": "Build check timed out (120s) - may indicate complex build"
          })
      except FileNotFoundError:
          result["skipped"] = True
          result["passed"] = True
          result["reason"] = "pip not available"
          result["info"].append({
              "type": "pip_unavailable",
              "message": "Build check skipped: pip not found",
              "severity": "INFO"
          })
          result["environment_limitation"] = True
      except Exception as e:
          result["warnings"].append({
              "type": "build_check_error",
              "message": f"Build check encountered error: {e}"
          })

      result["passed"] = len(result["errors"]) == 0
      print(json.dumps(result))
      EOF
    output: "build_check"
    parse_json: true
    timeout: 180
    depends_on: ["packaging-check"]

  - id: "set-default-build-check"
    type: "bash"
    condition: "{{packaging_check.has_pyproject}} != true"
    command: |
      python3 << 'EOF'
      import json
      result = {
          "phase": "build_check",
          "skipped": True,
          "passed": True,
          "reason": "No pyproject.toml - not a Python package"
      }
      print(json.dumps(result))
      EOF
    output: "build_check"
    parse_json: true
    timeout: 10
    depends_on: ["packaging-check"]

  # ============================================================================
  # PHASE 1: Enhanced Repository Discovery
  # Find all bundle files with separate tracking for each category
  # NEW in v3.0.0: Separate experiments tracking, token-based size analysis
  # ============================================================================

  - id: "repo-discovery"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      from pathlib import Path

      def estimate_tokens(content: str) -> int:
          """Estimate token count as len(content) // 4."""
          return len(content) // 4

      def discover_bundles(repo_path: str) -> dict:
          """Discover all bundle files in a repository with enhanced categorization."""
          results = {
              "phase": "discovery",
              "repo_path": repo_path,
              "bundles_found": {
                  "root": [],
                  "behaviors": [],
                  "standalone": [],  # /bundles/ directory
                  "experiments": [],  # /experiments/ directory - NEW in v3.0.0
                  "providers": [],
                  "agents": [],
                  "other": []
              },
              "bundle_details": {},  # path -> {size_bytes, size_tokens, ...}
              "total_count": 0,
              "repo_structure": {},
              "errors": []
          }

          path = Path(repo_path).resolve()

          if not path.exists():
              results["errors"].append({
                  "type": "path_error",
                  "message": f"Repository path does not exist: {repo_path}"
              })
              print(json.dumps(results))
              return results

          if not path.is_dir():
              results["errors"].append({
                  "type": "path_error",
                  "message": f"Path is not a directory: {repo_path}"
              })
              print(json.dumps(results))
              return results

          def get_bundle_details(bundle_path: Path) -> dict:
              """Get details about a bundle file including token count."""
              try:
                  content = bundle_path.read_text()
                  return {
                      "path": str(bundle_path),
                      "size_bytes": len(content),
                      "size_tokens": estimate_tokens(content),
                      "filename": bundle_path.name,
                      "parent_dir": bundle_path.parent.name
                  }
              except Exception as e:
                  return {
                      "path": str(bundle_path),
                      "size_bytes": 0,
                      "size_tokens": 0,
                      "error": str(e)
                  }

          # Check for root bundle
          for name in ["bundle.md", "bundle.yaml"]:
              root_bundle = path / name
              if root_bundle.exists():
                  results["bundles_found"]["root"].append(str(root_bundle))
                  results["bundle_details"][str(root_bundle)] = get_bundle_details(root_bundle)

          # Scan conventional directories with separate categories
          scan_dirs = {
              "behaviors": "behaviors",
              "standalone": "bundles",
              "experiments": "experiments",  # NEW in v3.0.0
              "providers": "providers",
              "agents": "agents"
          }

          for category, dir_name in scan_dirs.items():
              dir_path = path / dir_name
              if dir_path.exists() and dir_path.is_dir():
                  for item in dir_path.iterdir():
                      if item.is_file() and item.suffix in [".md", ".yaml", ".yml"]:
                          results["bundles_found"][category].append(str(item))
                          results["bundle_details"][str(item)] = get_bundle_details(item)
                      elif item.is_dir():
                          # Check for bundle inside subdirectory
                          for sub_name in ["bundle.md", "bundle.yaml"]:
                              sub_bundle = item / sub_name
                              if sub_bundle.exists():
                                  results["bundles_found"][category].append(str(sub_bundle))
                                  results["bundle_details"][str(sub_bundle)] = get_bundle_details(sub_bundle)

          # Map repo structure
          results["repo_structure"] = {
              "has_root_bundle": len(results["bundles_found"]["root"]) > 0,
              "has_behaviors": len(results["bundles_found"]["behaviors"]) > 0,
              "has_standalone": len(results["bundles_found"]["standalone"]) > 0,
              "has_experiments": len(results["bundles_found"]["experiments"]) > 0,
              "has_providers": len(results["bundles_found"]["providers"]) > 0,
              "has_agents": len(results["bundles_found"]["agents"]) > 0,
              "directories": [d.name for d in path.iterdir() if d.is_dir() and not d.name.startswith(".")]
          }

          # Count totals
          for category, bundles in results["bundles_found"].items():
              results["total_count"] += len(bundles)

          # Summary statistics
          results["summary"] = {
              "root_count": len(results["bundles_found"]["root"]),
              "behaviors_count": len(results["bundles_found"]["behaviors"]),
              "standalone_count": len(results["bundles_found"]["standalone"]),
              "experiments_count": len(results["bundles_found"]["experiments"]),
              "providers_count": len(results["bundles_found"]["providers"]),
              "agents_count": len(results["bundles_found"]["agents"]),
              "total_tokens": sum(d.get("size_tokens", 0) for d in results["bundle_details"].values())
          }

          print(json.dumps(results))
          return results

      discover_bundles("{{repo_path}}")
      EOF
    output: "discovery_results"
    parse_json: true
    timeout: 60
    depends_on: ["packaging-check", "build-check", "set-default-build-check"]

  # ============================================================================
  # PHASE 2: Individual Bundle Validation
  # Validate each discovered bundle structurally
  # ============================================================================

  - id: "validate-all-bundles"
    type: "bash"
    command: |
      python3 << 'EOF'
      import asyncio
      import json
      import sys
      from pathlib import Path

      env_check = json.loads('''{{env_check}}''')
      
      # v3.2.0: Check foundation availability from capabilities dict
      foundation_available = env_check.get("capabilities", {}).get("foundation_available", False)
      
      if not foundation_available:
          # v3.2.0: This is an environment limitation, NOT a bundle problem
          print(json.dumps({
              "phase": "individual_validation",
              "skipped": True,
              "environment_limitation": True,  # v3.2.0: Flag this as env issue
              "reason": "amplifier_foundation not available - BundleRegistry validation skipped",
              "validation_mode": env_check.get("validation_mode", "structural_only"),
              "note": "Structural checks still performed; install amplifier_foundation for full validation"
          }))
          sys.exit(0)

      from amplifier_foundation import BundleRegistry
      from amplifier_foundation.exceptions import (
          BundleLoadError,
          BundleNotFoundError,
          BundleDependencyError,
          BundleValidationError
      )

      discovery = json.loads('''{{discovery_results}}''')
      
      async def validate_bundle(bundle_path: str, category: str) -> dict:
          """Validate a single bundle and return results."""
          result = {
              "path": bundle_path,
              "category": category,
              "passed": True,
              "name": None,
              "errors": [],
              "warnings": []
          }

          try:
              registry = BundleRegistry()
              path = Path(bundle_path).resolve()
              uri = f"file://{path}"

              bundle = await registry._load_single(
                  uri,
                  auto_register=True,
                  auto_include=False
              )

              result["name"] = bundle.name
              result["version"] = bundle.version
              result["has_instruction"] = bool(bundle.instruction)
              result["includes_count"] = len(bundle.includes)
              result["agents_count"] = len(bundle.agents)
              result["has_description"] = bool(getattr(bundle, 'description', None))

              if not bundle.name:
                  result["warnings"].append("Missing bundle.name")
              
              if not getattr(bundle, 'description', None):
                  result["warnings"].append("Missing bundle description")

          except BundleDependencyError as e:
              result["passed"] = False
              result["errors"].append(f"Dependency error: {e}")
          except BundleLoadError as e:
              result["passed"] = False
              result["errors"].append(f"Load error: {e}")
          except Exception as e:
              result["passed"] = False
              result["errors"].append(f"{type(e).__name__}: {e}")

          return result

      async def validate_all():
          results = {
              "phase": "individual_validation",
              "skipped": False,
              "bundles": [],
              "summary": {
                  "total": 0,
                  "passed": 0,
                  "failed": 0,
                  "warnings": 0,
                  "by_category": {}
              }
          }

          all_bundles = []
          for category, paths in discovery.get("bundles_found", {}).items():
              for path in paths:
                  all_bundles.append((path, category))

          for bundle_path, category in all_bundles:
              result = await validate_bundle(bundle_path, category)
              results["bundles"].append(result)
              results["summary"]["total"] += 1
              
              if result["passed"]:
                  results["summary"]["passed"] += 1
              else:
                  results["summary"]["failed"] += 1
              
              if result.get("warnings"):
                  results["summary"]["warnings"] += len(result["warnings"])

              if category not in results["summary"]["by_category"]:
                  results["summary"]["by_category"][category] = {"passed": 0, "failed": 0, "warnings": 0}
              
              if result["passed"]:
                  results["summary"]["by_category"][category]["passed"] += 1
              else:
                  results["summary"]["by_category"][category]["failed"] += 1
              
              if result.get("warnings"):
                  results["summary"]["by_category"][category]["warnings"] += len(result["warnings"])

          print(json.dumps(results))

      asyncio.run(validate_all())
      EOF
    output: "individual_validation"
    parse_json: true
    timeout: 300
    on_error: "continue"
    depends_on: ["repo-discovery"]

  # ============================================================================
  # PHASE 2.1: Behavior Hygiene Validation (NEW in v3.0.0)
  # Validates bundles in /behaviors/ against hygiene rules:
  # - No includes to root bundles (anti-pattern)
  # - No session.orchestrator (behavior shouldn't be standalone)
  # - No providers (behavior shouldn't define providers)
  # - context.include total tokens < 2000 (ERROR) / < 1000 (WARNING)
  # - context.include file count <= 3 (WARNING if >3)
  # - Tool count <= 2 (WARNING if >2)
  # ============================================================================

  - id: "behavior-hygiene-validation"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      import re
      import sys
      from pathlib import Path

      def estimate_tokens(content: str) -> int:
          """Estimate token count as len(content) // 4."""
          return len(content) // 4

      def parse_yaml_bundle(path: Path) -> dict:
          """Parse a bundle file (YAML or MD with YAML frontmatter)."""
          try:
              content = path.read_text()
              
              # Handle markdown with YAML frontmatter
              if path.suffix == '.md':
                  # Extract YAML from frontmatter (between --- markers)
                  match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
                  if match:
                      yaml_content = match.group(1)
                  else:
                      return {"error": "No YAML frontmatter found"}
              else:
                  yaml_content = content
              
              # Try to parse YAML
              try:
                  import yaml
                  return yaml.safe_load(yaml_content) or {}
              except ImportError:
                  # Fallback: basic regex parsing for key fields
                  result = {}
                  
                  # Check for session.orchestrator
                  if re.search(r'^\s*orchestrator:', yaml_content, re.MULTILINE):
                      result["session"] = {"orchestrator": True}
                  
                  # Check for providers
                  if re.search(r'^providers:', yaml_content, re.MULTILINE):
                      result["providers"] = [{}]
                  
                  # Check for includes
                  includes_match = re.search(r'^includes:\s*\n((?:\s+-.*\n)*)', yaml_content, re.MULTILINE)
                  if includes_match:
                      includes_section = includes_match.group(1)
                      result["includes"] = re.findall(r'-\s*bundle:\s*(.+)', includes_section)
                  
                  # Check context.include
                  context_match = re.search(r'^context:\s*\n((?:\s+.*\n)*)', yaml_content, re.MULTILINE)
                  if context_match:
                      context_section = context_match.group(1)
                      include_match = re.search(r'include:\s*\n((?:\s+-.*\n)*)', context_section)
                      if include_match:
                          includes = re.findall(r'-\s*(.+)', include_match.group(1))
                          result["context"] = {"include": includes}
                  
                  # Check tools
                  tools_match = re.search(r'^tools:\s*\n((?:\s+-.*\n)*)', yaml_content, re.MULTILINE)
                  if tools_match:
                      tools_section = tools_match.group(1)
                      result["tools"] = [{}] * len(re.findall(r'-\s*module:', tools_section))
                  
                  return result
              
          except Exception as e:
              return {"error": str(e)}

      def is_root_bundle_reference(include_ref: str, repo_path: Path) -> bool:
          """Check if an include reference points to a root bundle."""
          # Known root bundles patterns
          root_patterns = [
              r'^foundation$',  # Just "foundation"
              r'amplifier-foundation(@|$)',  # GitHub reference to foundation
              r'bundle\.md$',  # Direct reference to bundle.md
              r'bundle\.yaml$',  # Direct reference to bundle.yaml
              r'^file://.*bundle\.(md|yaml)$',  # File URI to root bundle
          ]
          
          for pattern in root_patterns:
              if re.search(pattern, include_ref):
                  return True
          
          # Check if it's a reference to repo's own root bundle
          if 'bundle:' in include_ref:
              # Extract bundle name
              bundle_name = include_ref.replace('bundle:', '').strip()
              # Check if this resolves to repo root
              for ext in ['.md', '.yaml']:
                  if (repo_path / f"bundle{ext}").exists():
                      return bundle_name == '' or bundle_name == repo_path.name
          
          return False

      def validate_behaviors(repo_path: str) -> dict:
          """Validate all behaviors against hygiene rules."""
          results = {
              "phase": "behavior_hygiene",
              "behaviors_checked": 0,
              "errors": [],
              "warnings": [],
              "info": [],
              "behavior_details": []
          }

          path = Path(repo_path).resolve()
          behaviors_dir = path / "behaviors"

          if not behaviors_dir.exists():
              results["skipped"] = True
              results["passed"] = True
              results["reason"] = "No behaviors/ directory"
              print(json.dumps(results))
              return results

          # Collect all behavior files
          behavior_files = []
          for item in behaviors_dir.iterdir():
              if item.is_file() and item.suffix in [".md", ".yaml", ".yml"]:
                  behavior_files.append(item)
              elif item.is_dir():
                  for sub_name in ["bundle.md", "bundle.yaml"]:
                      sub_bundle = item / sub_name
                      if sub_bundle.exists():
                          behavior_files.append(sub_bundle)

          for behavior_path in behavior_files:
              results["behaviors_checked"] += 1
              behavior_name = behavior_path.stem if behavior_path.name not in ["bundle.md", "bundle.yaml"] else behavior_path.parent.name
              
              detail = {
                  "path": str(behavior_path),
                  "name": behavior_name,
                  "issues": []
              }

              # Parse the bundle
              bundle_data = parse_yaml_bundle(behavior_path)
              
              if "error" in bundle_data:
                  results["warnings"].append({
                      "behavior": behavior_name,
                      "type": "parse_error",
                      "message": f"Could not parse behavior: {bundle_data['error']}"
                  })
                  detail["parse_error"] = bundle_data["error"]
                  results["behavior_details"].append(detail)
                  continue

              # Rule 1: Check for root bundle includes (ERROR)
              includes = bundle_data.get("includes", [])
              if isinstance(includes, list):
                  for inc in includes:
                      if isinstance(inc, dict):
                          bundle_ref = inc.get("bundle", "")
                      else:
                          bundle_ref = str(inc)
                      
                      if is_root_bundle_reference(bundle_ref, path):
                          results["errors"].append({
                              "behavior": behavior_name,
                              "type": "includes_root_bundle",
                              "message": f"Behavior includes root bundle '{bundle_ref}' - anti-pattern",
                              "severity": "ERROR",
                              "fix": "Behaviors should not include root bundles. Extract shared functionality to a separate behavior."
                          })
                          detail["issues"].append("includes_root_bundle")

              # Rule 2: Check for session.orchestrator (ERROR)
              session = bundle_data.get("session", {})
              if isinstance(session, dict) and session.get("orchestrator"):
                  results["errors"].append({
                      "behavior": behavior_name,
                      "type": "has_session_orchestrator",
                      "message": "Behavior defines session.orchestrator - behaviors should not be standalone",
                      "severity": "ERROR",
                      "fix": "Remove session.orchestrator. If this needs to be standalone, move to /bundles/"
                  })
                  detail["issues"].append("has_session_orchestrator")

              # Rule 3: Check for providers (ERROR)
              providers = bundle_data.get("providers", [])
              if providers:
                  results["errors"].append({
                      "behavior": behavior_name,
                      "type": "has_providers",
                      "message": f"Behavior defines {len(providers)} provider(s) - behaviors should not define providers",
                      "severity": "ERROR",
                      "fix": "Remove providers. Providers should be in /providers/ or the root bundle."
                  })
                  detail["issues"].append("has_providers")

              # Rule 4: Check context.include token count
              context = bundle_data.get("context", {})
              context_includes = []
              if isinstance(context, dict):
                  context_includes = context.get("include", [])
              
              total_context_tokens = 0
              if context_includes:
                  detail["context_include_count"] = len(context_includes)
                  
                  # Estimate tokens for each included file
                  for include_ref in context_includes:
                      if isinstance(include_ref, str):
                          # Try to resolve and read the file
                          include_path = None
                          if include_ref.startswith("@"):
                              # Bundle reference - can't easily resolve without registry
                              # Estimate ~500 tokens per awareness file
                              total_context_tokens += 500
                          else:
                              # Relative path
                              include_path = behavior_path.parent / include_ref
                              if not include_path.exists():
                                  include_path = path / include_ref
                              
                              if include_path and include_path.exists():
                                  try:
                                      content = include_path.read_text()
                                      total_context_tokens += estimate_tokens(content)
                                  except:
                                      total_context_tokens += 500  # Default estimate
                              else:
                                  total_context_tokens += 500
                  
                  detail["context_total_tokens"] = total_context_tokens
                  
                  # ERROR: >2000 tokens
                  if total_context_tokens > 2000:
                      results["errors"].append({
                          "behavior": behavior_name,
                          "type": "context_tokens_excessive",
                          "message": f"context.include totals ~{total_context_tokens} tokens (>2000 ERROR threshold)",
                          "severity": "ERROR",
                          "fix": "Move heavy context to agents (context sink pattern). Behaviors should have minimal awareness-only context."
                      })
                      detail["issues"].append("context_tokens_excessive")
                  # WARNING: >1000 tokens
                  elif total_context_tokens > 1000:
                      results["warnings"].append({
                          "behavior": behavior_name,
                          "type": "context_tokens_high",
                          "message": f"context.include totals ~{total_context_tokens} tokens (>1000 WARNING threshold)",
                          "severity": "WARNING",
                          "fix": "Consider moving heavy context to agents. Behaviors should be thin."
                      })
                      detail["issues"].append("context_tokens_high")

                  # WARNING: >3 include files
                  if len(context_includes) > 3:
                      results["warnings"].append({
                          "behavior": behavior_name,
                          "type": "context_include_count_high",
                          "message": f"context.include has {len(context_includes)} files (>3 threshold)",
                          "severity": "WARNING",
                          "fix": "Consolidate context includes or move to agents."
                      })
                      detail["issues"].append("context_include_count_high")

              # Rule 5: Check tool count (WARNING if >2)
              tools = bundle_data.get("tools", [])
              if tools and len(tools) > 2:
                  results["warnings"].append({
                      "behavior": behavior_name,
                      "type": "tool_count_high",
                      "message": f"Behavior defines {len(tools)} tools (>2 threshold)",
                      "severity": "WARNING",
                      "fix": "Consider moving specialized tools behind agents."
                  })
                  detail["issues"].append("tool_count_high")
              
              detail["tool_count"] = len(tools) if tools else 0
              results["behavior_details"].append(detail)

          results["passed"] = len(results["errors"]) == 0
          results["summary"] = {
              "behaviors_checked": results["behaviors_checked"],
              "errors": len(results["errors"]),
              "warnings": len(results["warnings"]),
              "clean_behaviors": results["behaviors_checked"] - len(set(e["behavior"] for e in results["errors"]))
          }

          print(json.dumps(results))
          return results

      validate_behaviors("{{repo_path}}")
      EOF
    output: "behavior_hygiene_results"
    parse_json: true
    timeout: 120
    depends_on: ["validate-all-bundles"]

  # ============================================================================
  # PHASE 2.2: Standalone Completeness Validation (NEW in v3.0.0)
  # Validates bundles in /bundles/ have complete session configuration
  # (orchestrator + context module) either directly or via includes chain
  # ============================================================================

  - id: "standalone-completeness-validation"
    type: "bash"
    command: |
      python3 << 'EOF'
      import asyncio
      import json
      import re
      import sys
      from pathlib import Path

      env_check = json.loads('''{{env_check}}''')
      discovery = json.loads('''{{discovery_results}}''')

      results = {
          "phase": "standalone_completeness",
          "bundles_checked": 0,
          "errors": [],
          "warnings": [],
          "bundle_details": []
      }

      standalone_bundles = discovery.get("bundles_found", {}).get("standalone", [])

      if not standalone_bundles:
          results["skipped"] = True
          results["passed"] = True
          results["reason"] = "No bundles in /bundles/ directory"
          print(json.dumps(results))
          sys.exit(0)

      # v3.2.0: Check foundation from capabilities dict
      foundation_available = env_check.get("capabilities", {}).get("foundation_available", False)
      
      if not foundation_available:
          # v3.2.0: Fallback mode - basic YAML parsing without includes resolution
          results["environment_limitation"] = True
          results["info"] = results.get("info", [])
          results["info"].append({
              "type": "limited_validation",
              "message": "Foundation not available - includes chain not fully traced",
              "severity": "INFO"
          })
          
          for bundle_path in standalone_bundles:
              results["bundles_checked"] += 1
              path = Path(bundle_path)
              bundle_name = path.stem if path.name not in ["bundle.md", "bundle.yaml"] else path.parent.name
              
              detail = {
                  "path": bundle_path,
                  "name": bundle_name,
                  "has_orchestrator": False,
                  "has_context": False,
                  "has_includes": False,
                  "completeness": "unknown"
              }
              
              try:
                  content = path.read_text()
                  
                  # Check for session config
                  detail["has_orchestrator"] = bool(re.search(r'orchestrator:', content))
                  detail["has_context"] = bool(re.search(r'context:', content))
                  detail["has_includes"] = bool(re.search(r'^includes:', content, re.MULTILINE))
                  
                  if detail["has_orchestrator"] and detail["has_context"]:
                      detail["completeness"] = "direct"
                  elif detail["has_includes"]:
                      detail["completeness"] = "via_includes_unverified"
                      results["warnings"].append({
                          "bundle": bundle_name,
                          "type": "includes_not_traced",
                          "message": "Bundle has includes but cannot verify completeness without foundation"
                      })
                  else:
                      detail["completeness"] = "incomplete"
                      results["errors"].append({
                          "bundle": bundle_name,
                          "type": "missing_session_config",
                          "message": "Standalone bundle missing orchestrator/context and has no includes",
                          "severity": "ERROR",
                          "fix": "Add session.orchestrator and session.context, or include a bundle that provides them"
                      })
              except Exception as e:
                  detail["error"] = str(e)
                  results["warnings"].append({
                      "bundle": bundle_name,
                      "type": "read_error",
                      "message": f"Could not read bundle: {e}"
                  })
              
              results["bundle_details"].append(detail)
          
          results["passed"] = len(results["errors"]) == 0
          print(json.dumps(results))
          sys.exit(0)

      # Full validation with foundation available
      from amplifier_foundation import BundleRegistry

      async def check_completeness():
          for bundle_path in standalone_bundles:
              results["bundles_checked"] += 1
              path = Path(bundle_path).resolve()
              bundle_name = path.stem if path.name not in ["bundle.md", "bundle.yaml"] else path.parent.name
              
              detail = {
                  "path": bundle_path,
                  "name": bundle_name,
                  "has_orchestrator": False,
                  "has_context": False,
                  "via_includes": False,
                  "completeness": "unknown"
              }

              try:
                  registry = BundleRegistry()
                  uri = f"file://{path}"
                  
                  # Load with includes followed
                  bundle = await registry._load_single(
                      uri,
                      auto_register=True,
                      auto_include=True  # Follow includes
                  )
                  
                  # Check composed session config
                  session = getattr(bundle, 'session', None)
                  if session:
                      detail["has_orchestrator"] = bool(getattr(session, 'orchestrator', None))
                      detail["has_context"] = bool(getattr(session, 'context', None))
                  
                  # Check if bundle itself has orchestrator or got it via includes
                  content = path.read_text()
                  direct_orchestrator = bool(re.search(r'orchestrator:', content))
                  detail["via_includes"] = detail["has_orchestrator"] and not direct_orchestrator
                  
                  if detail["has_orchestrator"] and detail["has_context"]:
                      detail["completeness"] = "complete"
                  elif detail["has_orchestrator"]:
                      detail["completeness"] = "partial_missing_context"
                      results["errors"].append({
                          "bundle": bundle_name,
                          "type": "missing_context_module",
                          "message": "Standalone bundle has orchestrator but missing context module",
                          "severity": "ERROR",
                          "fix": "Add session.context or include a bundle that provides it"
                      })
                  elif detail["has_context"]:
                      detail["completeness"] = "partial_missing_orchestrator"
                      results["errors"].append({
                          "bundle": bundle_name,
                          "type": "missing_orchestrator",
                          "message": "Standalone bundle has context but missing orchestrator",
                          "severity": "ERROR",
                          "fix": "Add session.orchestrator or include a bundle that provides it"
                      })
                  else:
                      detail["completeness"] = "incomplete"
                      results["errors"].append({
                          "bundle": bundle_name,
                          "type": "missing_session_config",
                          "message": "Standalone bundle missing both orchestrator and context",
                          "severity": "ERROR",
                          "fix": "Add session configuration or include foundation/another complete bundle"
                      })
                  
                  # Note about provider (WARNING, not ERROR - intentional flexibility)
                  providers = getattr(bundle, 'providers', [])
                  if not providers:
                      results["warnings"].append({
                          "bundle": bundle_name,
                          "type": "no_provider",
                          "message": "Standalone bundle has no provider (may be intentional for flexibility)",
                          "severity": "INFO"
                      })
                      detail["has_provider"] = False
                  else:
                      detail["has_provider"] = True

              except Exception as e:
                  detail["error"] = str(e)
                  detail["completeness"] = "error"
                  results["errors"].append({
                      "bundle": bundle_name,
                      "type": "load_error",
                      "message": f"Could not load bundle: {e}",
                      "severity": "ERROR"
                  })
              
              results["bundle_details"].append(detail)

          results["passed"] = len(results["errors"]) == 0
          results["summary"] = {
              "bundles_checked": results["bundles_checked"],
              "complete": len([d for d in results["bundle_details"] if d.get("completeness") == "complete"]),
              "incomplete": len([d for d in results["bundle_details"] if d.get("completeness") in ["incomplete", "partial_missing_context", "partial_missing_orchestrator"]]),
              "errors": len(results["errors"]),
              "warnings": len(results["warnings"])
          }
          print(json.dumps(results))

      asyncio.run(check_completeness())
      EOF
    output: "standalone_completeness_results"
    parse_json: true
    timeout: 180
    depends_on: ["validate-all-bundles"]

  # ============================================================================
  # PHASE 2.3: Experiments Validation (NEW in v3.0.0)
  # Validates bundles in /experiments/:
  # - Name should be exp-* prefixed (WARNING)
  # - Description should include "EXPERIMENTAL" (WARNING)
  # - Must be complete (same as standalone) (ERROR)
  # ============================================================================

  - id: "experiments-validation"
    type: "bash"
    command: |
      python3 << 'EOF'
      import asyncio
      import json
      import re
      import sys
      from pathlib import Path

      env_check = json.loads('''{{env_check}}''')
      discovery = json.loads('''{{discovery_results}}''')

      results = {
          "phase": "experiments_validation",
          "experiments_checked": 0,
          "errors": [],
          "warnings": [],
          "experiment_details": []
      }

      experiment_bundles = discovery.get("bundles_found", {}).get("experiments", [])

      if not experiment_bundles:
          results["skipped"] = True
          results["passed"] = True
          results["reason"] = "No bundles in /experiments/ directory"
          print(json.dumps(results))
          sys.exit(0)

      for bundle_path in experiment_bundles:
          results["experiments_checked"] += 1
          path = Path(bundle_path)
          bundle_name = path.stem if path.name not in ["bundle.md", "bundle.yaml"] else path.parent.name
          
          detail = {
              "path": bundle_path,
              "name": bundle_name,
              "issues": []
          }
          
          try:
              content = path.read_text()
              
              # Rule 1: Name should be exp-* prefixed (WARNING)
              if not bundle_name.startswith("exp-"):
                  results["warnings"].append({
                      "experiment": bundle_name,
                      "type": "naming_convention",
                      "message": f"Experiment bundle '{bundle_name}' should be prefixed with 'exp-'",
                      "severity": "WARNING",
                      "fix": f"Rename to 'exp-{bundle_name}'"
                  })
                  detail["issues"].append("naming_convention")
              
              # Rule 2: Description should mention EXPERIMENTAL (WARNING)
              # Check for description field
              desc_match = re.search(r'description:\s*[|>]?\s*\n?\s*(.+?)(?=\n[a-z]|\n---|\Z)', content, re.IGNORECASE | re.DOTALL)
              if desc_match:
                  description = desc_match.group(1)
                  if "EXPERIMENTAL" not in description.upper():
                      results["warnings"].append({
                          "experiment": bundle_name,
                          "type": "missing_experimental_warning",
                          "message": "Experiment bundle description should include 'EXPERIMENTAL' warning",
                          "severity": "WARNING",
                          "fix": "Add 'EXPERIMENTAL' to the description to warn users"
                      })
                      detail["issues"].append("missing_experimental_warning")
                  detail["has_experimental_warning"] = "EXPERIMENTAL" in description.upper()
              else:
                  results["warnings"].append({
                      "experiment": bundle_name,
                      "type": "missing_description",
                      "message": "Experiment bundle has no description",
                      "severity": "WARNING"
                  })
                  detail["issues"].append("missing_description")
              
              # Rule 3: Must be complete (has orchestrator + context or includes)
              has_orchestrator = bool(re.search(r'orchestrator:', content))
              has_context = bool(re.search(r'^\s+context:', content, re.MULTILINE))
              has_includes = bool(re.search(r'^includes:', content, re.MULTILINE))
              
              detail["has_orchestrator"] = has_orchestrator
              detail["has_context"] = has_context
              detail["has_includes"] = has_includes
              
              if has_orchestrator and has_context:
                  detail["completeness"] = "direct"
              elif has_includes:
                  detail["completeness"] = "via_includes"
              else:
                  detail["completeness"] = "incomplete"
                  results["errors"].append({
                      "experiment": bundle_name,
                      "type": "incomplete_experiment",
                      "message": "Experiment bundle is not complete (missing session config and no includes)",
                      "severity": "ERROR",
                      "fix": "Add session.orchestrator and session.context, or include a complete bundle"
                  })
                  detail["issues"].append("incomplete_experiment")
              
          except Exception as e:
              detail["error"] = str(e)
              results["warnings"].append({
                  "experiment": bundle_name,
                  "type": "read_error",
                  "message": f"Could not read experiment bundle: {e}"
              })
          
          results["experiment_details"].append(detail)

      results["passed"] = len(results["errors"]) == 0
      results["summary"] = {
          "experiments_checked": results["experiments_checked"],
          "complete": len([d for d in results["experiment_details"] if d.get("completeness") in ["direct", "via_includes"]]),
          "incomplete": len([d for d in results["experiment_details"] if d.get("completeness") == "incomplete"]),
          "naming_issues": len([d for d in results["experiment_details"] if "naming_convention" in d.get("issues", [])]),
          "errors": len(results["errors"]),
          "warnings": len(results["warnings"])
      }

      print(json.dumps(results))
      EOF
    output: "experiments_validation_results"
    parse_json: true
    timeout: 120
    depends_on: ["validate-all-bundles"]

  # ============================================================================
  # PHASE 2.4: Context Sink Compliance (NEW in v3.0.0)
  # Validates root bundles follow context sink pattern:
  # - >5 context.include files in root → WARNING
  # - Any file >2000 tokens in root context → INFO (candidate for agent)
  # - Root owns >50% of total context tokens → INFO
  # ============================================================================

  - id: "context-sink-compliance"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      import re
      import sys
      from pathlib import Path

      def estimate_tokens(content: str) -> int:
          """Estimate token count as len(content) // 4."""
          return len(content) // 4

      discovery = json.loads('''{{discovery_results}}''')

      results = {
          "phase": "context_sink_compliance",
          "root_bundles_checked": 0,
          "errors": [],
          "warnings": [],
          "info": [],
          "root_details": []
      }

      root_bundles = discovery.get("bundles_found", {}).get("root", [])

      if not root_bundles:
          results["skipped"] = True
          results["passed"] = True
          results["reason"] = "No root bundle found"
          print(json.dumps(results))
          sys.exit(0)

      repo_path = Path(discovery.get("repo_path", ".")).resolve()

      for bundle_path in root_bundles:
          results["root_bundles_checked"] += 1
          path = Path(bundle_path)
          
          detail = {
              "path": bundle_path,
              "context_files": [],
              "total_context_tokens": 0,
              "issues": []
          }

          try:
              content = path.read_text()
              
              # Parse context.include section
              # Handle both YAML and MD formats
              if path.suffix == '.md':
                  # Extract YAML frontmatter
                  match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
                  if match:
                      yaml_content = match.group(1)
                  else:
                      yaml_content = ""
              else:
                  yaml_content = content
              
              # Find context.include entries
              context_match = re.search(r'context:\s*\n((?:\s+.*\n)*)', yaml_content)
              if context_match:
                  context_section = context_match.group(1)
                  include_match = re.search(r'include:\s*\n((?:\s+-.*\n)*)', context_section)
                  if include_match:
                      include_lines = include_match.group(1)
                      includes = re.findall(r'-\s*(.+)', include_lines)
                      
                      for include_ref in includes:
                          include_ref = include_ref.strip().strip('"').strip("'")
                          file_info = {
                              "reference": include_ref,
                              "resolved": False,
                              "tokens": 0
                          }
                          
                          # Try to resolve and read the file
                          if include_ref.startswith("@"):
                              # Bundle reference - estimate
                              file_info["tokens"] = 500
                              file_info["note"] = "Bundle reference - estimated"
                          else:
                              # Try relative paths
                              include_path = path.parent / include_ref
                              if not include_path.exists():
                                  include_path = repo_path / include_ref
                              
                              if include_path.exists():
                                  try:
                                      file_content = include_path.read_text()
                                      file_info["tokens"] = estimate_tokens(file_content)
                                      file_info["resolved"] = True
                                      file_info["resolved_path"] = str(include_path)
                                  except Exception as e:
                                      file_info["error"] = str(e)
                                      file_info["tokens"] = 500
                              else:
                                  file_info["note"] = "Could not resolve path"
                                  file_info["tokens"] = 500
                          
                          detail["context_files"].append(file_info)
                          detail["total_context_tokens"] += file_info["tokens"]
                          
                          # Check for large individual files (>2000 tokens)
                          if file_info["tokens"] > 2000:
                              results["info"].append({
                                  "root_bundle": path.name,
                                  "type": "large_context_file",
                                  "file": include_ref,
                                  "tokens": file_info["tokens"],
                                  "message": f"Context file '{include_ref}' has ~{file_info['tokens']} tokens - consider moving to agent",
                                  "severity": "INFO"
                              })
                              detail["issues"].append(f"large_file:{include_ref}")
                      
                      # Check total include count (>5 → WARNING)
                      if len(includes) > 5:
                          results["warnings"].append({
                              "root_bundle": path.name,
                              "type": "too_many_context_includes",
                              "count": len(includes),
                              "message": f"Root bundle has {len(includes)} context.include files (>5 threshold)",
                              "severity": "WARNING",
                              "fix": "Move specialized context to agents (context sink pattern)"
                          })
                          detail["issues"].append("too_many_context_includes")

              # Calculate total repo context tokens for ratio check
              total_repo_tokens = discovery.get("summary", {}).get("total_tokens", 0)
              if total_repo_tokens > 0 and detail["total_context_tokens"] > 0:
                  root_ratio = detail["total_context_tokens"] / total_repo_tokens
                  detail["root_context_ratio"] = round(root_ratio, 2)
                  
                  if root_ratio > 0.5:
                      results["info"].append({
                          "root_bundle": path.name,
                          "type": "high_root_context_ratio",
                          "ratio": round(root_ratio * 100),
                          "message": f"Root bundle owns {round(root_ratio * 100)}% of total context tokens",
                          "severity": "INFO",
                          "suggestion": "Consider distributing context to behaviors and agents"
                      })
                      detail["issues"].append("high_root_context_ratio")

          except Exception as e:
              detail["error"] = str(e)
              results["warnings"].append({
                  "root_bundle": path.name,
                  "type": "read_error",
                  "message": f"Could not analyze root bundle: {e}"
              })
          
          results["root_details"].append(detail)

      results["passed"] = len(results["errors"]) == 0
      results["summary"] = {
          "root_bundles_checked": results["root_bundles_checked"],
          "warnings": len(results["warnings"]),
          "info": len(results["info"]),
          "total_root_context_tokens": sum(d.get("total_context_tokens", 0) for d in results["root_details"])
      }

      print(json.dumps(results))
      EOF
    output: "context_sink_results"
    parse_json: true
    timeout: 120
    depends_on: ["validate-all-bundles"]

  # ============================================================================
  # PHASE 2.5: Tool Placement Analysis (Enhanced in v3.1.0)
  # Inheritance-aware tool placement analysis:
  # - Agents inherit ALL parent tools EXCEPT those in delegate's exclude_tools
  # - Default exclude_tools: ["delegate"] - only delegate is excluded by default
  # - If tool is in parent AND NOT excluded → agent declaration is REDUNDANT
  # - If tool is in parent AND excluded → agent declaration is CORRECT
  # - Universal tools (filesystem, bash, web, search, todo) → OK in root
  # - Specialized tools used by only one behavior → SUGGESTION to move to behavior
  # ============================================================================

  - id: "tool-placement-analysis"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      import re
      import sys
      from pathlib import Path

      discovery = json.loads('''{{discovery_results}}''')
      individual = json.loads('''{{individual_validation}}''')

      results = {
          "phase": "tool_placement_analysis",
          "tools_analyzed": 0,
          "errors": [],
          "warnings": [],
          "suggestions": [],
          "tool_inventory": [],
          "inheritance_analysis": {
              "exclude_tools": ["delegate"],  # Default
              "exclude_tools_source": "default",
              "agent_tool_analysis": []
          }
      }

      # Universal tools that are always OK in root
      UNIVERSAL_TOOLS = {
          "tool-filesystem", "filesystem",
          "tool-bash", "bash",
          "tool-web", "web",
          "tool-search", "search", "tool-grep", "grep", "tool-glob", "glob",
          "tool-todo", "todo",
          "tool-delegate", "delegate",
          "tool-recipes", "recipes",
          "tool-lsp", "lsp"
      }

      repo_path = Path(discovery.get("repo_path", ".")).resolve()

      # Collect all tools from all bundles and track agents
      tool_locations = {}  # tool_name -> [{"bundle": ..., "category": ..., "in_agent": ...}, ...]
      bundle_tools = {}    # bundle_path -> [tool_names]
      agent_tools = {}     # "bundle_path:agent_name" -> [tool_names]
      exclude_tools = ["delegate"]  # Default per foundation-expert
      
      all_bundles = []
      for category, paths in discovery.get("bundles_found", {}).items():
          for path in paths:
              all_bundles.append((path, category))

      def parse_yaml_section(content: str, section: str) -> str:
          """Extract a YAML section from content."""
          # Handle markdown with frontmatter
          if content.strip().startswith('---'):
              match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
              if match:
                  content = match.group(1)
          
          pattern = rf'^{section}:\s*\n((?:\s+.*\n)*)'
          match = re.search(pattern, content, re.MULTILINE)
          return match.group(1) if match else ""

      def extract_tool_modules(tools_section: str) -> list:
          """Extract tool module names from a tools section."""
          return re.findall(r'module:\s*([^\s\n]+)', tools_section)

      def extract_delegate_config(tools_section: str) -> dict:
          """Extract delegate tool config including exclude_tools."""
          config = {}
          # Look for tool-delegate section
          delegate_match = re.search(
              r'-\s*module:\s*tool-delegate.*?(?=\n\s*-\s*module:|\Z)',
              tools_section,
              re.DOTALL
          )
          if delegate_match:
              delegate_section = delegate_match.group(0)
              # Look for exclude_tools in config
              exclude_match = re.search(
                  r'exclude_tools:\s*\n((?:\s+-\s*.*\n)*)',
                  delegate_section
              )
              if exclude_match:
                  excludes = re.findall(r'-\s*([^\s\n]+)', exclude_match.group(1))
                  config["exclude_tools"] = excludes
          return config

      # First pass: collect bundle-level tools and find delegate config
      for bundle_path, category in all_bundles:
          path = Path(bundle_path)
          try:
              content = path.read_text()
              
              # Find tools section at bundle level
              tools_section = parse_yaml_section(content, "tools")
              if tools_section:
                  tool_modules = extract_tool_modules(tools_section)
                  bundle_tools[str(path)] = tool_modules
                  
                  for tool_name in tool_modules:
                      results["tools_analyzed"] += 1
                      
                      if tool_name not in tool_locations:
                          tool_locations[tool_name] = []
                      
                      tool_locations[tool_name].append({
                          "bundle": str(path),
                          "bundle_name": path.stem if path.name not in ["bundle.md", "bundle.yaml"] else path.parent.name,
                          "category": category,
                          "in_agent": None
                      })
                  
                  # Check for delegate tool config
                  if category in ["root", "behaviors"]:
                      delegate_config = extract_delegate_config(tools_section)
                      if "exclude_tools" in delegate_config:
                          exclude_tools = delegate_config["exclude_tools"]
                          results["inheritance_analysis"]["exclude_tools"] = exclude_tools
                          results["inheritance_analysis"]["exclude_tools_source"] = str(path)
              
              # Find agents section and their tools
              agents_section = parse_yaml_section(content, "agents")
              if agents_section:
                  # Split by agent entries
                  agent_entries = re.split(r'\n\s*-\s*(?=name:|id:)', agents_section)
                  for agent_entry in agent_entries:
                      if not agent_entry.strip():
                          continue
                      
                      # Get agent name
                      name_match = re.search(r'(?:name|id):\s*([^\s\n]+)', agent_entry)
                      if not name_match:
                          continue
                      agent_name = name_match.group(1).strip('"\'')
                      
                      # Get agent tools
                      agent_tools_match = re.search(r'tools:\s*\n((?:\s+.*\n)*)', agent_entry)
                      if agent_tools_match:
                          agent_tool_modules = extract_tool_modules(agent_tools_match.group(1))
                          agent_key = f"{path}:{agent_name}"
                          agent_tools[agent_key] = {
                              "tools": agent_tool_modules,
                              "bundle_path": str(path),
                              "bundle_category": category
                          }
                          
                          for tool_name in agent_tool_modules:
                              results["tools_analyzed"] += 1
                              
                              if tool_name not in tool_locations:
                                  tool_locations[tool_name] = []
                              
                              tool_locations[tool_name].append({
                                  "bundle": str(path),
                                  "bundle_name": path.stem if path.name not in ["bundle.md", "bundle.yaml"] else path.parent.name,
                                  "category": category,
                                  "in_agent": agent_name
                              })
                      
          except Exception as e:
              results["warnings"].append({
                  "bundle": str(path),
                  "type": "parse_error",
                  "message": f"Could not parse bundle for tools: {e}"
              })

      # Second pass: Analyze agent tool inheritance
      for agent_key, agent_info in agent_tools.items():
          bundle_path = agent_info["bundle_path"]
          agent_name = agent_key.split(":")[-1]
          agent_tool_list = agent_info["tools"]
          
          # Get parent bundle tools
          parent_tools = bundle_tools.get(bundle_path, [])
          
          # Also check root bundle tools if this is a behavior
          root_tools = []
          for root_path in discovery.get("bundles_found", {}).get("root", []):
              root_tools.extend(bundle_tools.get(root_path, []))
          
          # Combined parent tools (what agent inherits from)
          all_parent_tools = set(parent_tools) | set(root_tools)
          
          agent_analysis = {
              "agent": agent_name,
              "bundle": bundle_path,
              "declared_tools": agent_tool_list,
              "inherited_tools": list(all_parent_tools - set(exclude_tools)),
              "excluded_tools": exclude_tools,
              "redundant_declarations": [],
              "correct_explicit_declarations": []
          }
          
          for tool in agent_tool_list:
              if tool in all_parent_tools:
                  if tool in exclude_tools:
                      # Tool is excluded from inheritance - agent MUST declare it explicitly
                      agent_analysis["correct_explicit_declarations"].append(tool)
                  else:
                      # Tool is inherited - agent declaration is REDUNDANT
                      agent_analysis["redundant_declarations"].append(tool)
                      results["suggestions"].append({
                          "tool": tool,
                          "agent": agent_name,
                          "bundle": bundle_path,
                          "type": "redundant_agent_tool",
                          "message": f"Tool '{tool}' in agent '{agent_name}' is redundant - inherited from parent bundle",
                          "severity": "SUGGESTION",
                          "suggestion": f"Remove '{tool}' from agent tools list. It's inherited automatically (not in exclude_tools: {exclude_tools})"
                      })
          
          results["inheritance_analysis"]["agent_tool_analysis"].append(agent_analysis)

      # Analyze tool placement at bundle level
      for tool_name, locations in tool_locations.items():
          # Filter to bundle-level only (not in agents)
          bundle_locations = [loc for loc in locations if loc["in_agent"] is None]
          
          tool_info = {
              "name": tool_name,
              "is_universal": any(u in tool_name.lower() for u in UNIVERSAL_TOOLS),
              "locations": locations,
              "bundle_locations": bundle_locations,
              "location_count": len(locations),
              "bundle_location_count": len(bundle_locations),
              "in_root": any(loc["category"] == "root" and loc["in_agent"] is None for loc in locations),
              "in_behaviors": any(loc["category"] == "behaviors" and loc["in_agent"] is None for loc in locations),
              "in_agents": any(loc["in_agent"] is not None for loc in locations),
              "issues": []
          }
          
          # Check for non-universal tools in root that are only used by specific behaviors
          if tool_info["in_root"] and not tool_info["is_universal"]:
              behavior_users = [loc for loc in bundle_locations if loc["category"] == "behaviors"]
              if len(behavior_users) == 1:
                  results["suggestions"].append({
                      "tool": tool_name,
                      "type": "specialized_tool_in_root",
                      "message": f"Tool '{tool_name}' is in root but only used by behavior '{behavior_users[0]['bundle_name']}'",
                      "severity": "INFO",
                      "suggestion": "Consider moving to the specific behavior that uses it"
                  })
                  tool_info["issues"].append("specialized_in_root")
          
          # Note: We no longer flag root+behavior declarations as redundant without
          # understanding the full include chain. Behaviors may be used standalone.
          
          results["tool_inventory"].append(tool_info)

      # Summary statistics
      universal_count = len([t for t in results["tool_inventory"] if t["is_universal"]])
      specialized_count = len(results["tool_inventory"]) - universal_count
      redundant_agent_tools = sum(
          len(a.get("redundant_declarations", [])) 
          for a in results["inheritance_analysis"]["agent_tool_analysis"]
      )

      results["passed"] = len(results["errors"]) == 0
      results["summary"] = {
          "tools_analyzed": results["tools_analyzed"],
          "unique_tools": len(results["tool_inventory"]),
          "universal_tools": universal_count,
          "specialized_tools": specialized_count,
          "suggestions": len(results["suggestions"]),
          "warnings": len(results["warnings"]),
          "redundant_agent_tools": redundant_agent_tools,
          "exclude_tools_config": results["inheritance_analysis"]["exclude_tools"]
      }

      print(json.dumps(results))
      EOF
    output: "tool_placement_results"
    parse_json: true
    timeout: 120
    depends_on: ["validate-all-bundles"]

  # ============================================================================
  # PHASE 2.6: Quality Classification (Deterministic)
  # Enhanced in v3.0.0 to incorporate all new validation phases
  # ============================================================================

  - id: "quality-classification"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      import sys

      # Parse all results
      individual = json.loads('''{{individual_validation}}''')
      discovery = json.loads('''{{discovery_results}}''')
      packaging = json.loads('''{{packaging_check}}''')
      
      # v3.2.0: Parse environment check for environment status tracking
      env_check_raw = '''{{env_check}}'''
      try:
          env_check = json.loads(env_check_raw)
      except (json.JSONDecodeError, ValueError):
          env_check = {"capabilities": {}, "limitations": [], "install_instructions": [], "validation_mode": "unknown"}
      
      build_check_raw = '''{{build_check}}'''
      try:
          build = json.loads(build_check_raw)
      except (json.JSONDecodeError, ValueError):
          build = {"passed": True, "skipped": True, "reason": "build check output not available"}

      # NEW in v3.0.0: Parse new validation phase results
      behavior_hygiene_raw = '''{{behavior_hygiene_results}}'''
      try:
          behavior_hygiene = json.loads(behavior_hygiene_raw)
      except (json.JSONDecodeError, ValueError):
          behavior_hygiene = {"passed": True, "skipped": True}

      standalone_raw = '''{{standalone_completeness_results}}'''
      try:
          standalone = json.loads(standalone_raw)
      except (json.JSONDecodeError, ValueError):
          standalone = {"passed": True, "skipped": True}

      experiments_raw = '''{{experiments_validation_results}}'''
      try:
          experiments = json.loads(experiments_raw)
      except (json.JSONDecodeError, ValueError):
          experiments = {"passed": True, "skipped": True}

      context_sink_raw = '''{{context_sink_results}}'''
      try:
          context_sink = json.loads(context_sink_raw)
      except (json.JSONDecodeError, ValueError):
          context_sink = {"passed": True, "skipped": True}

      tool_placement_raw = '''{{tool_placement_results}}'''
      try:
          tool_placement = json.loads(tool_placement_raw)
      except (json.JSONDecodeError, ValueError):
          tool_placement = {"passed": True, "skipped": True}

      # Handle case where individual validation was skipped
      if individual.get("skipped", False):
          # v3.2.0: Check if this is an environment limitation (not a bundle problem)
          is_env_limitation = individual.get("environment_limitation", False)
          
          if is_env_limitation:
              # Environment limitation - NOT critical, just limited validation
              result = {
                  "phase": "quality_classification",
                  "quality_level": "limited",  # v3.2.0: New level for env limitations
                  "requires_llm_analysis": True,
                  "environment_limitation": True,
                  "validation_mode": individual.get("validation_mode", "structural_only"),
                  "skipped_reason": individual.get("reason", "validation skipped"),
                  "bundles": [],
                  "summary": {
                      "total": discovery.get("total_count", 0),
                      "good": 0,
                      "polish": 0,
                      "needs_work": 0,
                      "critical": 0,
                      "not_validated": discovery.get("total_count", 0)
                  },
                  "structural_issues": [],
                  "packaging_issues": [],
                  "hygiene_issues": [],
                  "completeness_issues": [],
                  "environment_status": {
                      "foundation_available": False,
                      "hatchling_available": env_check.get("capabilities", {}).get("hatchling_available", False),
                      "limitations": env_check.get("limitations", []),
                      "install_instructions": env_check.get("install_instructions", [])
                  },
                  "message": f"ℹ️ Limited validation mode: {individual.get('reason', 'foundation not available')}. Structural checks passed."
              }
          else:
              # Actual validation failure (not env related)
              result = {
                  "phase": "quality_classification",
                  "quality_level": "critical",
                  "requires_llm_analysis": True,
                  "skipped_reason": individual.get("reason", "validation skipped"),
                  "bundles": [],
                  "summary": {
                      "total": 0,
                      "good": 0,
                      "polish": 0,
                      "needs_work": 0,
                      "critical": 0
                  },
                  "structural_issues": [{
                      "type": "validation_skipped",
                      "message": individual.get("reason", "Individual bundle validation was skipped"),
                      "severity": "critical"
                  }],
                  "packaging_issues": [],
                  "hygiene_issues": [],
                  "completeness_issues": [],
                  "message": f"⚠️ Bundle validation could not run: {individual.get('reason', 'unknown reason')}"
              }
          print(json.dumps(result))
          sys.exit(0)

      def classify_bundle(bundle: dict) -> dict:
          """Classify a single bundle's quality level."""
          has_errors = len(bundle.get("errors", [])) > 0
          has_warnings = len(bundle.get("warnings", [])) > 0
          has_description = bundle.get("has_description", False)
          
          if has_errors:
              quality = "critical"
              reason = f"Bundle failed to load: {bundle.get('errors', ['unknown'])[0]}"
          elif has_warnings:
              quality = "polish"
              reason = f"Has {len(bundle.get('warnings', []))} warnings"
          else:
              quality = "good"
              reason = "Loads successfully with no issues"
          
          return {
              "path": bundle.get("path"),
              "name": bundle.get("name"),
              "category": bundle.get("category"),
              "quality": quality,
              "reason": reason,
              "has_description": has_description,
              "error_count": len(bundle.get("errors", [])),
              "warning_count": len(bundle.get("warnings", []))
          }

      def classify_repo():
          results = {
              "phase": "classification",
              "bundles": [],
              "summary": {
                  "total": 0,
                  "good": 0,
                  "polish": 0,
                  "needs_work": 0,
                  "critical": 0
              },
              "quality_level": "good",
              "requires_llm_analysis": False,
              "structural_issues": [],
              "packaging_issues": [],
              "hygiene_issues": [],
              "completeness_issues": [],
              "experiment_issues": [],
              "context_sink_issues": [],
              "tool_placement_issues": [],
              # v3.2.0: Environment status tracking
              "environment_status": {
                  "validation_mode": env_check.get("validation_mode", "unknown"),
                  "foundation_available": env_check.get("capabilities", {}).get("foundation_available", False),
                  "hatchling_available": env_check.get("capabilities", {}).get("hatchling_available", False),
                  "limitations": env_check.get("limitations", []),
                  "full_validation": env_check.get("summary", {}).get("full_validation_available", False)
              },
              "environment_info": []  # v3.2.0: Info messages about env limitations
          }
          
          # Classify each bundle
          for bundle in individual.get("bundles", []):
              classification = classify_bundle(bundle)
              results["bundles"].append(classification)
              results["summary"]["total"] += 1
              results["summary"][classification["quality"]] += 1
          
          # Check structural requirements
          repo_structure = discovery.get("repo_structure", {})
          has_root = repo_structure.get("has_root_bundle", False)
          has_behaviors = repo_structure.get("has_behaviors", False)
          has_standalone = repo_structure.get("has_standalone", False)
          has_agents = repo_structure.get("has_agents", False)
          
          if not has_root and not has_behaviors and not has_standalone:
              results["structural_issues"].append({
                  "type": "no_entry_point",
                  "message": "No root bundle, behaviors/, or bundles/ directory found",
                  "severity": "needs_work"
              })
          
          if has_agents and not has_behaviors and not has_root:
              results["structural_issues"].append({
                  "type": "orphan_agents",
                  "message": "agents/ directory exists but no behaviors or root bundle to include them",
                  "severity": "needs_work"
              })
          
          # Check packaging errors
          if not packaging.get("passed", True):
              for error in packaging.get("errors", []):
                  results["packaging_issues"].append({
                      "type": error.get("type", "packaging_error"),
                      "message": error.get("message", "Unknown packaging error"),
                      "severity": "critical",
                      "fix": error.get("fix", "Check pyproject.toml configuration")
                  })
          
          if not build.get("passed", True):
              for error in build.get("errors", []):
                  results["packaging_issues"].append({
                      "type": error.get("type", "build_error"),
                      "message": error.get("message", "Unknown build error"),
                      "severity": "critical",
                      "details": error.get("details", "")[:500]
                  })
          
          # v3.2.0: Track build environment limitations (INFO, not errors)
          if build.get("environment_limitation", False):
              for info in build.get("info", []):
                  results["environment_info"].append({
                      "type": info.get("type", "env_info"),
                      "message": info.get("message", "Environment limitation"),
                      "severity": "INFO",
                      "suggestion": info.get("suggestion", "")
                  })

          # NEW in v3.0.0: Check hygiene errors
          if not behavior_hygiene.get("skipped", False) and not behavior_hygiene.get("passed", True):
              for error in behavior_hygiene.get("errors", []):
                  results["hygiene_issues"].append({
                      "type": error.get("type"),
                      "behavior": error.get("behavior"),
                      "message": error.get("message"),
                      "severity": error.get("severity", "ERROR"),
                      "fix": error.get("fix", "")
                  })
          
          # Add hygiene warnings
          for warning in behavior_hygiene.get("warnings", []):
              results["hygiene_issues"].append({
                  "type": warning.get("type"),
                  "behavior": warning.get("behavior"),
                  "message": warning.get("message"),
                  "severity": "WARNING"
              })

          # Check standalone completeness errors
          if not standalone.get("skipped", False) and not standalone.get("passed", True):
              for error in standalone.get("errors", []):
                  results["completeness_issues"].append({
                      "type": error.get("type"),
                      "bundle": error.get("bundle"),
                      "message": error.get("message"),
                      "severity": error.get("severity", "ERROR")
                  })

          # Check experiments errors
          if not experiments.get("skipped", False) and not experiments.get("passed", True):
              for error in experiments.get("errors", []):
                  results["experiment_issues"].append({
                      "type": error.get("type"),
                      "experiment": error.get("experiment"),
                      "message": error.get("message"),
                      "severity": error.get("severity", "ERROR")
                  })
          
          # Add experiment warnings
          for warning in experiments.get("warnings", []):
              results["experiment_issues"].append({
                  "type": warning.get("type"),
                  "experiment": warning.get("experiment"),
                  "message": warning.get("message"),
                  "severity": "WARNING"
              })

          # Context sink issues (warnings/info only)
          for warning in context_sink.get("warnings", []):
              results["context_sink_issues"].append({
                  "type": warning.get("type"),
                  "message": warning.get("message"),
                  "severity": "WARNING"
              })
          for info in context_sink.get("info", []):
              results["context_sink_issues"].append({
                  "type": info.get("type"),
                  "message": info.get("message"),
                  "severity": "INFO"
              })

          # Tool placement suggestions
          for suggestion in tool_placement.get("suggestions", []):
              results["tool_placement_issues"].append({
                  "type": suggestion.get("type"),
                  "tool": suggestion.get("tool"),
                  "message": suggestion.get("message"),
                  "severity": "INFO"
              })

          # Determine overall quality level
          # Priority: critical > needs_work > polish > good
          
          critical_count = results["summary"]["critical"]
          critical_count += len([i for i in results["packaging_issues"] if i.get("severity") == "critical"])
          critical_count += len([i for i in results["hygiene_issues"] if i.get("severity") == "ERROR"])
          critical_count += len([i for i in results["completeness_issues"] if i.get("severity") == "ERROR"])
          critical_count += len([i for i in results["experiment_issues"] if i.get("severity") == "ERROR"])
          
          needs_work_count = results["summary"]["needs_work"]
          needs_work_count += len([i for i in results["structural_issues"] if i.get("severity") == "needs_work"])
          
          warning_count = results["summary"]["polish"]
          warning_count += len([i for i in results["hygiene_issues"] if i.get("severity") == "WARNING"])
          warning_count += len([i for i in results["context_sink_issues"] if i.get("severity") == "WARNING"])
          warning_count += len([i for i in results["experiment_issues"] if i.get("severity") == "WARNING"])
          
          if critical_count > 0:
              results["quality_level"] = "critical"
              results["requires_llm_analysis"] = True
          elif needs_work_count > 0:
              results["quality_level"] = "needs_work"
              results["requires_llm_analysis"] = True
          elif warning_count > 0:
              results["quality_level"] = "polish"
              results["requires_llm_analysis"] = True
          else:
              results["quality_level"] = "good"
              results["requires_llm_analysis"] = False
          
          # Summary message
          total = results["summary"]["total"]
          good = results["summary"]["good"]
          
          if results["quality_level"] == "good":
              results["message"] = f"✅ All {total} bundles meet quality thresholds. No further analysis needed."
          else:
              issues = []
              if critical_count > 0:
                  issues.append(f"{critical_count} critical")
              if needs_work_count > 0:
                  issues.append(f"{needs_work_count} need work")
              if warning_count > 0:
                  issues.append(f"{warning_count} warnings")
              
              results["message"] = f"⚠️ Issues found: {', '.join(issues)}. {good}/{total} bundles are good."
          
          # v3.0.0 hygiene summary
          results["hygiene_summary"] = {
              "behavior_hygiene_passed": behavior_hygiene.get("passed", True),
              "standalone_completeness_passed": standalone.get("passed", True),
              "experiments_passed": experiments.get("passed", True),
              "total_hygiene_errors": len(results["hygiene_issues"]) + len(results["completeness_issues"]) + len(results["experiment_issues"]),
              "context_sink_warnings": len([i for i in results["context_sink_issues"] if i.get("severity") == "WARNING"]),
              "tool_placement_suggestions": len(results["tool_placement_issues"])
          }
          
          # v3.2.0: Add environment limitations summary
          results["environment_summary"] = {
              "validation_mode": results["environment_status"]["validation_mode"],
              "full_validation_available": results["environment_status"]["full_validation"],
              "limitations_count": len(results["environment_status"]["limitations"]),
              "environment_info_count": len(results["environment_info"])
          }
          
          print(json.dumps(results))

      classify_repo()
      EOF
    output: "quality_classification"
    parse_json: true
    timeout: 60
    depends_on: ["validate-all-bundles", "behavior-hygiene-validation", "standalone-completeness-validation", "experiments-validation", "context-sink-compliance", "tool-placement-analysis"]

  # ============================================================================
  # PHASE 2.75: Initialize Optional Outputs
  # ============================================================================

  - id: "initialize-optional-outputs"
    type: "bash"
    command: |
      python3 << 'EOF'
      import json
      
      defaults = {
          "approval_summary": "_Quick approval not performed - detailed LLM analysis was run instead._",
          "composition_analysis": "_Composition analysis not performed - all bundles met quality thresholds via deterministic checks._",
          "repo_conventions": "_Convention analysis not performed - all bundles met quality thresholds._"
      }
      
      print(json.dumps(defaults))
      EOF
    output: "optional_defaults"
    parse_json: true
    timeout: 30
    depends_on: ["quality-classification"]

  - id: "set-default-approval-summary"
    type: "bash"
    command: |
      echo "_Quick approval not performed - detailed LLM analysis was run instead._"
    output: "approval_summary"
    timeout: 10
    depends_on: ["initialize-optional-outputs"]

  - id: "set-default-composition-analysis"
    type: "bash"
    command: |
      echo "_Composition analysis not performed - all bundles met quality thresholds via deterministic checks._"
    output: "composition_analysis"
    timeout: 10
    depends_on: ["initialize-optional-outputs"]

  - id: "set-default-repo-conventions"
    type: "bash"
    command: |
      echo "_Convention analysis not performed - all bundles met quality thresholds._"
    output: "repo_conventions"
    timeout: 10
    depends_on: ["initialize-optional-outputs"]

  # ============================================================================
  # PHASE 3: Quick Approval (When All Bundles Pass)
  # ============================================================================

  - id: "quick-approval"
    agent: "foundation:zen-architect"
    mode: "REVIEW"
    provider: "anthropic"
    model: "claude-haiku"
    condition: "{{quality_classification.requires_llm_analysis}} == false"
    depends_on: ["set-default-approval-summary"]
    prompt: |
      All bundles in this repository meet quality thresholds. Provide a brief approval summary.

      **Repository Path:** {{repo_path}}

      **Quality Classification Results:**
      ```json
      {{quality_classification}}
      ```

      **Discovery Summary:**
      ```json
      {{discovery_results}}
      ```

      **Packaging Check:**
      ```json
      {{packaging_check}}
      ```

      **Build Check:**
      ```json
      {{build_check}}
      ```

      **Hygiene Summary (v3.0.0):**
      - Behavior hygiene: {{quality_classification.hygiene_summary.behavior_hygiene_passed}}
      - Standalone completeness: {{quality_classification.hygiene_summary.standalone_completeness_passed}}
      - Experiments: {{quality_classification.hygiene_summary.experiments_passed}}

      Provide a concise approval noting:
      1. ✅ Confirm all bundles load and meet quality standards
      2. ✅ Confirm Python packaging is valid (if applicable)
      3. ✅ Confirm bundle hygiene (behaviors thin, standalones complete, context sink pattern)
      4. What's done well (patterns worth preserving)
      5. Any optional polish suggestions (NOT requirements - bundles are already good)

      Keep the response brief - this is the fast-track approval path.
      
      Format:
      ## ✅ PASS - All Bundles Meet Quality Standards
      
      **Summary:** [1-2 sentences]
      
      **What's Done Well:**
      - [list 2-3 positive patterns]
      
      **Optional Polish (not required):**
      - [0-2 minor suggestions, or "None - repository is exemplary"]
    output: "approval_summary"
    timeout: 120

  # ============================================================================
  # PHASE 4: Repository Composition Analysis (Conditional)
  # ============================================================================

  - id: "composition-analysis"
    agent: "foundation:zen-architect"
    mode: "ANALYZE"
    condition: "{{quality_classification.requires_llm_analysis}} == true"
    depends_on: ["set-default-composition-analysis"]
    prompt: |
      Analyze the composition of this bundle repository with focus on hygiene violations.

      **Repository Path:** {{repo_path}}

      **Quality Classification (focus on these issues):**
      ```json
      {{quality_classification}}
      ```

      **Discovery Results:**
      ```json
      {{discovery_results}}
      ```

      **Behavior Hygiene Results:**
      ```json
      {{behavior_hygiene_results}}
      ```

      **Standalone Completeness Results:**
      ```json
      {{standalone_completeness_results}}
      ```

      **Experiments Validation Results:**
      ```json
      {{experiments_validation_results}}
      ```

      **Context Sink Compliance:**
      ```json
      {{context_sink_results}}
      ```

      **Tool Placement Analysis:**
      ```json
      {{tool_placement_results}}
      ```

      **IMPORTANT: Focus on actual issues identified in the validation phases above.**

      **v3.0.0 Hygiene Rules to Enforce:**

      1. **Behavior Hygiene (ERROR if violated):**
         - Behaviors must NOT include root bundles (context bloat anti-pattern)
         - Behaviors must NOT have session.orchestrator (not standalone)
         - Behaviors must NOT define providers
         - context.include tokens must be <2000 (ERROR) / <1000 (WARNING)
         - Behaviors should have ≤3 context.include files
         - Behaviors should have ≤2 tools

      2. **Standalone Completeness (ERROR if violated):**
         - Bundles in /bundles/ MUST have orchestrator + context (direct or via includes)
         - Missing provider is WARNING only (intentional flexibility)

      3. **Experiments Rules:**
         - Should be exp-* prefixed (WARNING)
         - Description should include EXPERIMENTAL (WARNING)
         - Must be complete like standalone bundles (ERROR)

      4. **Context Sink Pattern (INFO/WARNING):**
         - Root with >5 context.include files → WARNING
         - Individual files >2000 tokens → INFO (candidate for agent)
         - Root owns >50% of context → INFO

      5. **Tool Placement (INFO/SUGGESTION) - v3.1.0 Inheritance-Aware:**
         - Universal tools OK in root
         - Specialized tools used by one behavior → suggest moving to behavior
         - Agent tool declarations checked against inheritance:
           - Tools in exclude_tools (default: ["delegate"]) → agent MUST declare (CORRECT)
           - Tools NOT in exclude_tools → agent declaration is REDUNDANT (SUGGESTION to remove)

      **What is NOT an Issue:**
      - Using /bundles/ vs /behaviors/ is a LOCATION choice
      - Alternative valid patterns that still work
      - Personal preferences about organization

      **Output Format:**
      For each validation domain with issues:
      - **Domain**: behavior_hygiene | standalone_completeness | experiments | context_sink | tool_placement
      - **Finding**: What was observed
      - **Status**: ERROR | WARNING | INFO
      - **Fix**: How to resolve
    output: "composition_analysis"
    timeout: 300
    depends_on: ["quality-classification"]

  # ============================================================================
  # PHASE 4.5: Convention Compliance (Conditional)
  # ============================================================================

  - id: "repo-conventions"
    agent: "foundation:zen-architect"
    mode: "REVIEW"
    condition: "{{quality_classification.requires_llm_analysis}} == true"
    depends_on: ["set-default-repo-conventions", "composition-analysis"]
    prompt: |
      Review repository-wide convention compliance, incorporating v3.0.0 hygiene findings.

      **Repository Path:** {{repo_path}}

      **Quality Classification:**
      ```json
      {{quality_classification}}
      ```

      **Discovery:**
      ```json
      {{discovery_results}}
      ```

      **Focus on genuine convention violations that impact functionality.**

      **Conventions to Check:**

      1. **Directory Structure:**
         - /bundle.md or /bundle.yaml at root
         - /behaviors/ for capability bundles (should be thin!)
         - /bundles/ for standalone variant bundles (must be complete!)
         - /experiments/ for experimental bundles (prefixed exp-*)
         - /providers/ for provider configurations
         - /agents/ for agent definitions
         - /context/ for shared context files

      2. **Bundle Hygiene (v3.0.0):**
         - Behaviors: thin, no root includes, no session config, limited context
         - Standalones: complete session config (direct or via includes)
         - Experiments: prefixed, warned, complete

      3. **Context Sink Pattern:**
         - Heavy context should be in agents, not root/behaviors
         - Root should have awareness-level context only
         - Specialized knowledge → agents

      4. **Tool Placement:**
         - Universal tools (filesystem, bash, web, search) → root OK
         - Capability tools → behaviors
         - Specialized tools → behind agents

      **Output Format:**
      Convention checklist with PASS/FAIL/N/A and notes.
      Only list FAIL items if they genuinely break functionality or violate hygiene rules.
    output: "repo_conventions"
    timeout: 300
    depends_on: ["composition-analysis"]

  # ============================================================================
  # PHASE 5: Final Report Synthesis
  # Enhanced in v3.0.0 with hygiene sections
  # ============================================================================

  - id: "synthesize-report"
    agent: "foundation:zen-architect"
    mode: "ANALYZE"
    prompt: |
      Synthesize all validation results into a comprehensive repository validation report.

      **Repository:** {{repo_path}}

      **Quality Classification:**
      ```json
      {{quality_classification}}
      ```

      **Environment Prerequisites (v3.2.0):**
      ```json
      {{env_check}}
      ```
      
      **IMPORTANT v3.2.0 - Environment Limitations:**
      - If `validation_mode` != "full", some checks were skipped due to missing tools
      - Environment limitations are INFO, not errors - they don't affect the bundle quality
      - Report clearly what was validated vs what was skipped

      **Packaging Check:**
      ```json
      {{packaging_check}}
      ```

      **Build Check:**
      ```json
      {{build_check}}
      ```

      **Discovery:**
      ```json
      {{discovery_results}}
      ```

      **Individual Bundle Validation:**
      ```json
      {{individual_validation}}
      ```

      **Behavior Hygiene (v3.0.0):**
      ```json
      {{behavior_hygiene_results}}
      ```

      **Standalone Completeness (v3.0.0):**
      ```json
      {{standalone_completeness_results}}
      ```

      **Experiments Validation (v3.0.0):**
      ```json
      {{experiments_validation_results}}
      ```

      **Context Sink Compliance (v3.0.0):**
      ```json
      {{context_sink_results}}
      ```

      **Tool Placement Analysis (v3.0.0):**
      ```json
      {{tool_placement_results}}
      ```

      **Quick Approval Summary:**
      {{approval_summary}}

      **Composition Analysis:**
      {{composition_analysis}}

      **Repository Conventions:**
      {{repo_conventions}}

      **REPORT GENERATION RULES:**

      1. If quality_level == "good": Generate a PASS report celebrating clean bill of health
      2. If quality_level == "polish": Generate a PASS WITH SUGGESTIONS report
      3. If quality_level == "needs_work": Generate a PASS WITH WARNINGS report
      4. If quality_level == "critical": Generate a FAIL report

      **Generate this report structure:**

      # Bundle Repository Validation Report v3.2.0

      ## Executive Summary
      - **Overall Verdict**: [✅ PASS | ✅ PASS WITH SUGGESTIONS | ⚠️ PASS WITH WARNINGS | ℹ️ LIMITED VALIDATION | ❌ FAIL]
      - **Repository**: path
      - **Validation Mode**: full | full_no_build | hygiene_only | structural_only (v3.2.0)
      - **Bundles Found**: X total (Y root, Z behaviors, W standalone, V experiments, ...)
      - **Quality Breakdown**: X good, Y polish, Z needs_work, W critical
      - **Packaging**: PASS/FAIL/SKIPPED
      - **Hygiene**: PASS/FAIL (v3.0.0 checks)
      - **Issues**: X errors, Y warnings, Z suggestions

      ## Environment Status (v3.2.0)
      
      **Always include this section showing what validation level was achieved:**
      
      | Capability | Status | Impact |
      |------------|--------|--------|
      | amplifier_foundation | ✅/❌ | BundleRegistry validation |
      | hatchling | ✅/❌ | Python package builds |
      | pip wheel | ✅/❌ | Build dry-run |
      
      **Validation Mode**: [mode] - [description]
      
      **If any limitations exist:**
      - What checks were skipped
      - Clarify: "Environment limitations are NOT bundle problems"
      - Include clear install instructions from `install_instructions` in env_check:
      
      ### Enable Full Validation
      
      Install missing dependencies, then **re-run this recipe** for complete validation.
      
      **Standard pip (if allowed):**
      ```bash
      [commands from install_instructions.standard]
      ```
      
      **PEP 668 systems (externally-managed Python):**
      ```bash
      [commands from install_instructions.pep668]
      ```
      
      > After installing, re-run: `amplifier tool invoke recipes operation=execute recipe_path=foundation:recipes/validate-bundle-repo.yaml context='{"repo_path": "/path/to/repo"}'`

      ## Bundle Discovery
      Summary of what was found in the repository by category.

      ## Python Packaging Status
      (if applicable)

      ## Quality Classification Summary

      | Bundle | Category | Quality | Issues |
      |--------|----------|---------|--------|
      | name   | category | level   | count  |

      ## v3.0.0 Hygiene Validation

      ### Behavior Hygiene
      | Behavior | Issues | Status |
      |----------|--------|--------|
      
      Issues:
      - [ERROR] Root bundle includes
      - [ERROR] Session config in behavior
      - [ERROR] Providers in behavior
      - [ERROR] context.include >2000 tokens
      - [WARNING] context.include >1000 tokens
      - [WARNING] >3 context.include files
      - [WARNING] >2 tools

      ### Standalone Completeness
      | Bundle | Orchestrator | Context | Provider | Status |
      |--------|--------------|---------|----------|--------|

      ### Experiments Validation
      | Experiment | Naming | Warning | Complete | Status |
      |------------|--------|---------|----------|--------|

      ### Context Sink Compliance
      - Root context files: X
      - Root context tokens: Y
      - Large files (>2000 tokens): [list]
      - Root context ratio: Z%

      ### Tool Placement (v3.1.0 Inheritance-Aware)
      - Universal tools: X
      - Specialized tools: Y
      - exclude_tools config: [list from delegate tool, default: ["delegate"]]
      - Redundant agent tool declarations: X (tools agents declare but already inherit)
      - Suggestions: [list]

      ## Detailed Findings

      **For PASS verdict:** Highlight what's done well.

      **For other verdicts:**

      ### Errors (Must Fix) - HIGH Priority
      Hygiene violations, completeness failures, load errors

      ### Warnings (Should Fix) - MEDIUM Priority
      Context bloat, naming conventions, structural issues

      ### Suggestions (Consider) - LOW Priority
      Tool placement optimizations, context sink improvements

      ## Recommendations

      Prioritized list of actions with clear HIGH/MEDIUM/LOW labels.

      ## Metadata
      - Validated: [timestamp]
      - Recipe: validate-bundle-repo v3.2.0
      - Validation Mode: [mode from env_check]
      - Foundation: [version or "not available"]
      - Quality Thresholds:
        - All bundles must load via BundleRegistry
        - Behaviors must be thin (no root includes, limited context)
        - Standalones must be complete (orchestrator + context)
        - Context sink pattern enforced (heavy context → agents)
        - Python packaging must build successfully (if present)
    output: "final_report"
    timeout: 300
    depends_on: ["quality-classification", "quick-approval", "composition-analysis", "repo-conventions", "set-default-approval-summary", "set-default-composition-analysis", "set-default-repo-conventions"]
